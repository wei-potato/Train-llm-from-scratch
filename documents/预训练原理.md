## 训练所需要的流程详解


## 参数传递
用两个类，每个类定义类变量对应传入的参数即可解析
ModelArguments
DataTrainingArguments


## 数据处理
加载tokenizer
#### 数据格式
{"text": "..."}

## 模型加载
model = AutoModelForCausalLM.from_config(config, trust_remote_code=True)
config 里可以定义模型的参数大小
比如 max_seq_len = 512
    dim = 512
    n_layers = 8
    n_heads = 8
则模型大小可以缩小为一个不到1b的模型，可以在4台3090 训练

transformers库 集成了 deepspeed 也继承了模型加载，只要在配置文件里
写好配置参数 比如：
  "architectures": [
    "LlamaForCausalLM"
  ]
则自动加载llama架构

## 损失函数和训练
损失函数是交叉熵损失，整个训练过程都被transformers集成
好处是代码比较简单，缺点是报错很难定位解决

